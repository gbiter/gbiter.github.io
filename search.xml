<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[知识图谱中的关系预测模型-二维卷积神经网络词向量]]></title>
    <url>%2F2018%2F06%2F13%2Fknowledge-graph-embedding%2F</url>
    <content type="text"><![CDATA[前言 在语言模型中传统的方式都是使用马尔科夫或者隐马尔科夫模型来对语言建模，随着神经网络的兴起，循环神经网络在语言处理中发挥着举足轻重的作用，这两种模型，马尔科夫模型和循环神经网络，可以从语言的语序关系中根据上一时刻的词义，词意，词性等学习之前状态的信息来预测下一个单词或者词组可能出现的概率，语言由多个相互联系的词序组成，马尔科夫模型和RNN正好可以满足这种需求，很少有人尝试使用CNN来解决语言语义的问题，而在知识图谱中，知识作为实体-关系-实体的三元组模式，在知识图谱解决关系预测这个问题的时候卷积神经网络可以起到很好的作用，因为关系和实体是相互依存的关系。他们之间的关系不再是语言模型中严格的词序关系，而是相互之间对等的关系，因此作者想到了使用连接的方式将他们链接起来，然后通过卷积神经网络学习实体-关系之间的特征，根据根据这些特征学习模型的训练参数，最终使用训练好的模型测试训练结果。 已存在的模型 [DistMult][1] (Yang et al. 2015)也是一种知识图谱中的关系预测模型，DistMult是一个浅层模型，可以学习到的参数有限，每个参数就代表一个特征，要想使得模型能具有表现力，只能是增加特征，而增加特征就要使用学习更多的参数，这样就会增加词向量的维度，这样的话对于更大规模的知识图谱，这种方式就不会很理想。要解决大规模的问题，一方面需要提高参数的利用率，另一方面选择一种更快的计算操作，而卷积就可以满足这种要求。 一维卷积和二维卷积交互表现 在自然语言处理中使用卷积操作的情况很少，语言模型中大多关注的词序列的先后顺序性，很少使用像卷积这样的操作，但是在知识图谱的三元组中，实体-关系-实体之间的关系不在关系他们之间的的顺序问题，而主要关注的是他们之间的关系问题，因此我们只要可以将它们之间的关系尽可能的表达出来，所以卷积操作就可以在知识图谱中使用。到底是使用一维卷积还是二维卷积。 $$( \begin{bmatrix} a &amp;a&a;&amp; b &amp;b&amp;b\end{bmatrix}) = \begin{bmatrix} a&amp;a&amp;a&amp;b&amp;b&amp;b\end{bmatrix}$$ 一维的卷积操作无法充分提取两个实体向量$A$和$B$的交互性的特征，因此需要可以使用二维卷积的方式，这样不仅可以解决词向量维度的问题还可以尽可能地提取到两个实体或者实体和关系之间的特征。这样就可以充分地在神经网络中进行训练最终对关系进行预测。 为了使得两个词向量之间的交互性更强，使得它们更能表现出它们之间的关系特征，将实体词向量和关系词向量重新进行拼接。如下图所示： 这样可以充分地提取到实体词向量和关系词向量之间的特征，为后面预测与另一个实体是否存在关系奠定了很好的基础。 每一个输入实体与一个高维的向量相关联，或者使用的是’one-hot’的索引向量或者’n-hot’的特征向量。我们使用$\chi_{e_1}$, $\chi_{e_2}$表示实体$e_1$, $e_2$。 [1]: Yang B, Yih W, He X, et al. Embedding Entities and Relations for Learning and Inference in Knowledge Bases[J]. eprint arXiv:1412.6575.]]></content>
      <tags>
        <tag>CNN</tag>
        <tag>Embedding</tag>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[出口]]></title>
    <url>%2F2018%2F06%2F12%2Fexit-1%2F</url>
    <content type="text"><![CDATA[最近心情比较烦躁，做什么事都提不起精神，时不时地抽风，时不时莫名的没了归属感，不知道自己来这里是干啥来了，就想着当一天和尚撞一天钟，那要真想这样我又为何这样的，还不如算了。回去吧！回去你自己甘心吗，回去你就可以心安理得吗，不能，那就脚踏实地的奋斗青春，不然又何必这么折腾自己呢，想想起初。不忘初心，方得始终，我的初心是什么，或许我真的需要好好问问自己，问问自己的内心，我的初心是什么，是什么让我处在现在这样一种境地，是什么让我不在珍惜已有的生活，是什么让我忘了过去，是什么让我不再憧憬未来，生活是自己努力的结果，不是信天游的得过且过，好好想想，应该怎样给自己树立点信心，给自己设定点目标。 前几天和同事聊天，同事说我现在极端，突然我也发现自己很极端，很讨人厌，为什么，因为我偏离了自己，偏激地看待周围的人和事，失去了耐心，丢失了宽容，放弃了初心，虽然还不知道自己的初心是什么，但是已经被我践踏地七零八落。 曾记得在开学典礼上有个同学发了个状态：生在一个伟大的时代，学在伟大的大学。那时候信心满满，憧憬无限。刚刚经历了不到一年的时间，曾经的那份热情，那份激扬文字的心气有始无终地消散，面对各种问题还是选择逃避拖延，缺乏了那份主动，还处在一种填鸭式的学习方式。感觉渐渐地与社会疏远了。因此觉得有必要写点什么？生活，杂记，专业……。给心灵找一个出口，这个出口不是为了别的，为了自己健康地活下去，我突然发现为什么大学生会出现这样或者那样的问题，因此长时间处在一个环境中，只有输入，而没有输出，也就是无论生活还是专业都缺少出口，缺少一个说话的空间，虽然现代社交给予和很多的选择，但网络交流的方式只是暂时地缓解这种压力，而真正的出口需要自己能理解自己，所以我开始觉得写作太重要了，写点东西记录生活和学习的历程，或许没人看，也或许只有三五个人浏览，我觉得已经足够了，因为写不是为了别人，而是为了自己，抒发自己的情感，流露自己的心声，为心灵找一个出口，心烦时打开读读自己的过去，看看自己书写的历史，就可以回顾自己，读自己的历史。 寻找自己，应该有一个平台，文字的平台，不是语音平台。更不是抖音，还有其他什么乱七八糟的各种平台，像知乎，微博，简书这样的地方让我去书写自己的历史，书写自己的人生，以前都是看别人写的，看别人心灵鸡汤，现在感觉自己也需要有一个自己的心灵鸡汤来安慰自己浮躁不安的心，给自己一方净土，给心灵一方安宁，让她在安宁中书写自己，书写心灵。其实觉得现在的每个人都需要有这样的一方净土，或许有三两个同病相怜的知音可能会去看看我写的内容，但已经够了，写是为了自己，不是为了别人，当下很火的短视频，各种终端随意地拍几张照片都可以诠释生活的种种，但静下心来会发现这些记忆就像快餐文化一样，其实就是快餐文化，渐渐吞噬着我平静的心灵，已经吞噬地浮躁不安了，不能继续下去任由其疯狂了。]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>Myself</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown_syntax]]></title>
    <url>%2F2018%2F06%2F12%2Fmarkdown-syntax%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F06%2F12%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
